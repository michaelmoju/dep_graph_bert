{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ujson\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange as trange\n",
    "from collections import OrderedDict\n",
    "\n",
    "w_dir = %pwd\n",
    "work_dir = os.path.dirname(w_dir) + '/../..'\n",
    "work_dir\n",
    "sys.path.append(work_dir + \"/dep_graph_bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dep_graph_bert.dataset_readers.dgb import DgbReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import TextFieldTensors, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/moju/data/work/dep_graph_bert/dep_graph_bert/dataset_readers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers = {'bert': PretrainedTransformerMismatchedIndexer('bert-base-uncased', namespace = 'token')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DgbReader(token_indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142edcea726b4b2ba8c163da0289b9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Progâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = reader.read(file_path = \"/media/moju/data/work/dep_graph_bert/acl-14-short-data/test.raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 24 with text: \n",
      " \t\t[ , shaquille, o'neal,  , to, miss, 3rd, straight, playoff, game, |, the, ..., :, $, t$, will, miss,\n",
      "\t\this, third, straight, play, ..., .]\n",
      " \t\tand TokenIndexers : {'bert': 'PretrainedTransformerMismatchedIndexer'} \n",
      " \t adj_matrix: ArrayField with shape: (24, 24) and dtype: <class 'numpy.float32'>. \n",
      " \t aspect_span: SpanField with spans: (0, 1). \n",
      " \t meta: {'comment_text': '$T$ to miss 3rd straight playoff game | The ... : $T$ will miss his third straight play ... .', 'aspect': \"shaquille o'neal\"} \n",
      " \t label: LabelField with label: -1 in namespace: 'labels'.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(texts,aspect):\n",
    "    source=''\n",
    "    splitnum=0\n",
    "    haha=[]\n",
    "    for i,text in enumerate(texts):\n",
    "        source+=text\n",
    "        splitnum+=len(tokenize(text))\n",
    "        haha.extend(tokenize(text))\n",
    "        if i <len(texts)-1:\n",
    "            source+=' '+aspect+' '\n",
    "            splitnum+=len(tokenize(aspect))\n",
    "    if splitnum!=len(tokenize(source.strip())):\n",
    "        print(haha)\n",
    "        print(texts)\n",
    "        print(aspect)\n",
    "        print(source)\n",
    "        print(splitnum)\n",
    "        print(tokenize(source.strip()))\n",
    "        print(len(tokenize(source.strip())))\n",
    "        a=input('gfg')\n",
    "    return re.sub(r' {2,}',' ',source.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'to miss 3rd straight playoff game | The ... : $T$ will miss his third straight play ... .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to miss 3rd straight playoff game | the ... :',\n",
       " 'will miss his third straight play ... .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.lower().strip() for s in text.split(\"$T$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3ec6db6bc4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$T$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"shaquille o'neal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext_raw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8304327f0f4b>\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(texts, aspect)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msplitnum\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mhaha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "out = \"\"\n",
    "text_left = [s.lower().strip() for s in text.split(\"$T$\")]\n",
    "aspect = \"shaquille o'neal\"\n",
    "text_raw = concat(text_left,aspect)\n",
    "out += text_raw + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = data[1]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_lengths = text_field.get_padding_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert': {'token_ids': tensor([  101,  6203,  6796,  1010,  3897,  7415,  3111,  2357,  2046,  5898,\n",
       "          17764,  1010,  1005,  1005,  1037,  2100,   999,  1005,  1005,  2357,\n",
       "           2046,  1005,  1005, 25430,  8490,  1005,  1005,  1010,  1057,  3473,\n",
       "           2039,  1010,  1998,  2057,  1005,  2310,  2042,  2182,  2035,  1996,\n",
       "           2126,  1012, 19387,  1045,  1057,  2293, 12170, 22669,  1625,   102]),\n",
       "  'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True]),\n",
       "  'type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'wordpiece_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True]),\n",
       "  'offsets': tensor([[ 1,  1],\n",
       "          [-1, -1],\n",
       "          [ 2,  2],\n",
       "          [-1, -1],\n",
       "          [ 3,  3],\n",
       "          [ 4,  4],\n",
       "          [ 5,  6],\n",
       "          [ 7,  7],\n",
       "          [ 8,  8],\n",
       "          [ 9,  9],\n",
       "          [10, 10],\n",
       "          [11, 11],\n",
       "          [12, 13],\n",
       "          [14, 15],\n",
       "          [16, 16],\n",
       "          [17, 18],\n",
       "          [19, 19],\n",
       "          [20, 20],\n",
       "          [21, 22],\n",
       "          [23, 24],\n",
       "          [25, 26],\n",
       "          [27, 27],\n",
       "          [28, 28],\n",
       "          [29, 29],\n",
       "          [30, 30],\n",
       "          [31, 31],\n",
       "          [32, 32],\n",
       "          [33, 33],\n",
       "          [34, 34],\n",
       "          [35, 35],\n",
       "          [36, 36],\n",
       "          [37, 37],\n",
       "          [38, 38],\n",
       "          [39, 39],\n",
       "          [40, 40],\n",
       "          [41, 41],\n",
       "          [42, 42],\n",
       "          [43, 43],\n",
       "          [44, 44],\n",
       "          [45, 45],\n",
       "          [46, 47],\n",
       "          [48, 48]])}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.as_tensor(padding_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
